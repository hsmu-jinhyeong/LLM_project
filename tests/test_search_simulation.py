"""
ì‹¤ì œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜: ë¹„ê±´ ë‹¨ë°±ì§ˆ ê²€ìƒ‰ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""
import sys
import os
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
load_dotenv(project_root / ".env")

# API í‚¤ í™•ì¸
if not os.getenv('OPENAI_API_KEY'):
    print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
    print("   .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    sys.exit(1)

sys.path.insert(0, str(project_root / "menu_bot_phase1"))

from recipe_retriever import create_recipe_retriever
import pandas as pd
import numpy as np

# 1. ë°ì´í„° ë¡œë“œ
DATA_PATH = project_root / "data" / "recipe_full_with_embeddings.parquet"
print(f"ë°ì´í„° ë¡œë”© ì‹œë„: {DATA_PATH}")

try:
    df = pd.read_parquet(DATA_PATH)
    print(f"âœ… Parquet ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ ë ˆì‹œí”¼\n")
except Exception as e:
    print(f"âŒ Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ CSV ì‚¬ìš©...\n")
    df = pd.read_csv("data/TB_RECIPE_SEARCH_241226.csv", encoding='utf-8')
    print(f"âœ… CSV ë¡œë“œ ì„±ê³µ: {len(df)}ê°œ ë ˆì‹œí”¼\n")

# 2. ì„ë² ë”© í™•ì¸
if 'embedding' in df.columns:
    print(f"âœ… ì„ë² ë”© ì»¬ëŸ¼ ì¡´ì¬")
    embeddings = np.array(df['embedding'].tolist(), dtype='float32')
    print(f"   ì„ë² ë”© shape: {embeddings.shape}")
else:
    print(f"âŒ ì„ë² ë”© ì»¬ëŸ¼ ì—†ìŒ! ì»¬ëŸ¼: {df.columns.tolist()[:10]}...")
    print("   ì„ë² ë”© ì—†ì´ ê²€ìƒ‰ ë¶ˆê°€ëŠ¥")
    sys.exit(1)

# 3. Retriever ìƒì„±
print(f"\n{'='*60}")
print("Retriever ìƒì„± ì¤‘...")
print(f"{'='*60}")

try:
    retriever = create_recipe_retriever(
        df=df,
        embeddings_array=embeddings,
        top_k=10
    )
    print(f"âœ… Retriever ìƒì„± ì„±ê³µ\n")
except Exception as e:
    print(f"âŒ Retriever ìƒì„± ì‹¤íŒ¨: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
test_queries = [
    "ë¹„ê±´ ì‹ë‹¨ì¸ë° ë‹¨ë°±ì§ˆ ë³´ì¶©í•  ìˆ˜ ìˆëŠ” ìš”ë¦¬ ì¶”ì²œí•´ì¤˜",
    "ë‘ë¶€ ìš”ë¦¬",
    "ì½©ë‚˜ë¬¼ ë ˆì‹œí”¼",
]

for query in test_queries:
    print(f"\n{'='*60}")
    print(f"ê²€ìƒ‰ì–´: {query}")
    print(f"{'='*60}")
    
    try:
        # ë¹„ê±´ í‚¤ì›Œë“œ ê°ì§€
        is_vegan_query = any(k in query.lower() for k in ['ë¹„ê±´', 'vegan', 'ì±„ì‹'])
        
        # ì¿¼ë¦¬ ê°œì„ : ë¹„ê±´ + ë‹¨ë°±ì§ˆì´ë©´ ì‹ë¬¼ì„± ë‹¨ë°±ì§ˆ í‚¤ì›Œë“œ ì¶”ê°€
        enhanced_query = query
        if is_vegan_query and any(k in query.lower() for k in ['ë‹¨ë°±ì§ˆ', 'ì˜ì–‘']):
            enhanced_query = f"{query} ë‘ë¶€ ì½© ê²¬ê³¼ë¥˜"
            print(f"ğŸ” ì¿¼ë¦¬ ê°•í™”: {enhanced_query}\n")
        
        search_k = 50 if is_vegan_query else 10
        
        # vectorstore ì§ì ‘ ì‚¬ìš© (workflowì™€ ë™ì¼)
        docs = retriever.vectorstore.similarity_search(enhanced_query, k=search_k)
        
        print(f"âœ… ê²€ìƒ‰ ì„±ê³µ: {len(docs)}ê°œ ê²°ê³¼ (k={search_k})\n")
        
        # ë¹„ê±´ í•„í„°ë§
        vegan_exclude = [
            # ìœ¡ë¥˜
            'ê³ ê¸°', 'ì‡ ê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ë‹­ê³ ê¸°', 'ë‹­', 'ì–‘ê³ ê¸°', 'ì˜¤ë¦¬ê³ ê¸°', 'ì‚¼ê²©ì‚´', 'ëª©ì‚´', 'í•­ì •ì‚´', 'ë“±ì‹¬', 'ì•ˆì‹¬', 'ê°ˆë¹„', 'ì°¨ëŒ', 'ì‚¬íƒœ', 'ì–‘ì§€', 'ìš°ì‚¼ê²©',
            # ë‹¬ê±€/ìœ ì œí’ˆ
            'ë‹¬ê±€', 'ê³„ë€', 'ì¹˜ì¦ˆ', 'ë²„í„°', 'ìš°ìœ ', 'í¬ë¦¼', 'ìš”êµ¬ë¥´íŠ¸', 'ìƒí¬ë¦¼', 'ë…¸ë¥¸ì', 'í°ì',
            # í•´ì‚°ë¬¼ (ì „ì²´)
            'ìƒì„ ', 'í•´ì‚°ë¬¼', 'ë‹¤ìŠ¬ê¸°', 'êµ´', 'ì¡°ê°œ', 'ìƒˆìš°', 'ê²Œ', 'ì˜¤ì§•ì–´', 'ë‚™ì§€', 'ë¬¸ì–´', 'ì£¼ê¾¸ë¯¸',
            'ê³ ë“±ì–´', 'ê°ˆì¹˜', 'ê½ì¹˜', 'ì°¸ì¹˜', 'ì—°ì–´', 'ê´‘ì–´', 'ìš°ëŸ­', 'ì¡°ê¸°', 'ë©¸ì¹˜', 'ë¶ì–´', 'ëª…íƒœ', 'ëŒ€êµ¬', 'ë™íƒœ',
            'ì¡°ê°¯ì‚´', 'í™í•©', 'ë°”ì§€ë½', 'ê°€ë¦¬ë¹„', 'ì „ë³µ', 'ì†Œë¼', 'í•´ë¬¼', 'ì–´ë¬µ', 'ì˜¤ë±', 'ê³¨ë±…ì´',
            # íŠ¹ìˆ˜ ë™ë¬¼ì„±
            'ì„ ì§€', 'ê³±ì°½', 'ë§‰ì°½', 'ëª…ë€', 'ì°½ë€', 'ì•Œíƒ•', 'ì “ê°ˆ', 'ê¹Œë‚˜ë¦¬', 'ì•¡ì “',
            # ê°€ê³µìœ¡
            'ë² ì´ì»¨', 'ì†Œì‹œì§€', 'í–„', 'ìŠ¤íŒ¸', 'ìœ¡í¬', 'ë² ì»¨'
        ]
        
        filtered = []
        for doc in docs:
            title = doc.metadata.get('title', '')
            content = doc.page_content
            
            if not any(excluded in title + content for excluded in vegan_exclude):
                filtered.append(doc)
        
        print(f"í•„í„°ë§ í›„: {len(filtered)}ê°œ (ì œê±°: {len(docs) - len(filtered)}ê°œ)\n")
        
        if filtered:
            print("ë¹„ê±´ ë ˆì‹œí”¼:")
            for i, doc in enumerate(filtered[:5], 1):
                print(f"  {i}. {doc.metadata.get('title', 'N/A')}")
        else:
            print("âŒ í•„í„°ë§ í›„ ë ˆì‹œí”¼ ì—†ìŒ!")
            print("\nì œê±°ëœ ë ˆì‹œí”¼ (ë¹„ê±´ ì•„ë‹˜):")
            for i, doc in enumerate(docs[:5], 1):
                title = doc.metadata.get('title', 'N/A')
                found = [e for e in vegan_exclude if e in title + doc.page_content]
                print(f"  {i}. {title}")
                if found:
                    print(f"     ê¸ˆì§€ì¬ë£Œ: {', '.join(found[:3])}...")
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

print(f"\n{'='*60}")
print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
print(f"{'='*60}")
